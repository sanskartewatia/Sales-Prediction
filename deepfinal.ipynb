{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "top = np.percentile(train.Purchase,99.5)\n",
    "train = train[ (train['Purchase']<top) ]\n",
    "#test1.head()\n",
    "train.fillna(-999,inplace=True)\n",
    "test.fillna(-999,inplace=True)\n",
    "dl = pd.DataFrame()\n",
    "dl['User_ID'] = test['User_ID']\n",
    "dl['Product_ID'] = test['Product_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col=[\"User_ID\", \"Product_ID\"]\n",
    "for var in cat_col:\n",
    "    lb = preprocessing.LabelEncoder()\n",
    "    full_var_data = pd.concat((train[var], test[var]), axis=0).astype('str')\n",
    "    lb.fit(full_var_data)\n",
    "    train[var] = lb.transform(train[var].astype('str'))\n",
    "    test[var] = lb.transform(test[var].astype('str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'City_Category', 'Stay_In_Current_City_Years']\n"
     ]
    }
   ],
   "source": [
    "new_gender = {'F':0, 'M':1}\n",
    "train.Gender.replace(new_gender,inplace=True)\n",
    "test.Gender.replace(new_gender,inplace=True)\n",
    "# Get  columns whose data type is object\n",
    "filteredColumns = train.dtypes[train.dtypes == np.object]\n",
    "# list of columns whose data type is object\n",
    "listOfColumnNames = list(filteredColumns.index)\n",
    "print(listOfColumnNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in listOfColumnNames:\n",
    "    if i!=0:\n",
    "        train=pd.get_dummies(train, columns=[i])\n",
    "        test=pd.get_dummies(test, columns=[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_to_category_map = {}\n",
    "customer_purchase_power = train.groupby(\"User_ID\")[\"Purchase\"].sum()\n",
    "#customer_purchase_power.median()\n",
    "\n",
    "values = customer_purchase_power.iteritems()\n",
    "for key, val in values:\n",
    "    \n",
    "    if val <= 143662.0:\n",
    "        user_id_to_category_map[key] = 1\n",
    "    if val <= 202156.0:\n",
    "        user_id_to_category_map[key] = 2\n",
    "    elif val <= 275408.0:\n",
    "        user_id_to_category_map[key] = 3\n",
    "    elif val <= 378451.0:\n",
    "        user_id_to_category_map[key] = 4\n",
    "    elif val <= 513874.0:\n",
    "        user_id_to_category_map[key] = 5\n",
    "    elif val <= 687241.0:\n",
    "        user_id_to_category_map[key] = 6\n",
    "    elif val <= 934097.0:\n",
    "        user_id_to_category_map[key] = 7\n",
    "    elif val <= 1332584.0:\n",
    "        user_id_to_category_map[key] = 8\n",
    "    elif val <= 2029891.0:\n",
    "        user_id_to_category_map[key] = 9\n",
    "    else:\n",
    "        user_id_to_category_map[key] = 5\n",
    "customer_purchase_power.median()\n",
    "def get_customer_category(user_id):\n",
    "    if user_id in user_id_to_category_map:\n",
    "        return user_id_to_category_map[user_id]\n",
    "    return 5\n",
    "train[\"buying_power\"] = list(map(lambda user_id: get_customer_category(user_id), train[\"User_ID\"]))\n",
    "test[\"buying_power\"] = list(map(lambda user_id: get_customer_category(user_id), test[\"User_ID\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"User_ID_MinPrice\"] = train.groupby(['User_ID'])['Purchase'].transform('min')\n",
    "userID_min_dict = train.groupby(['User_ID'])['Purchase'].min().to_dict()\n",
    "test['User_ID_MinPrice'] = test['User_ID'].apply(lambda x:userID_min_dict.get(x,0))\n",
    "\n",
    "train[\"User_ID_MaxPrice\"] = train.groupby(['User_ID'])['Purchase'].transform('max')\n",
    "userID_max_dict = train.groupby(['User_ID'])['Purchase'].max().to_dict()\n",
    "test['User_ID_MaxPrice'] = test['User_ID'].apply(lambda x:userID_max_dict.get(x,0))\n",
    "\n",
    "train[\"Product_ID_MinPrice\"] = train.groupby(['Product_ID'])['Purchase'].transform('min')\n",
    "productID_min_dict = train.groupby(['Product_ID'])['Purchase'].min().to_dict()\n",
    "test['Product_ID_MinPrice'] = test['Product_ID'].apply(lambda x:productID_min_dict.get(x,0))\n",
    "\n",
    "train[\"Product_ID_MaxPrice\"] = train.groupby(['Product_ID'])['Purchase'].transform('max')\n",
    "productID_max_dict = train.groupby(['Product_ID'])['Purchase'].max().to_dict()\n",
    "test['Product_ID_MaxPrice'] = test['Product_ID'].apply(lambda x:productID_max_dict.get(x,0))\n",
    "\n",
    "train[\"User_ID_Count\"] = train.groupby(['User_ID'])['User_ID'].transform('count')\n",
    "uID_count = train.groupby(['User_ID']).size().to_dict()\n",
    "test['User_ID_Count'] = test['User_ID'].apply(lambda x:uID_count.get(x,0))\n",
    "\n",
    "train[\"Product_ID_Count\"] = train.groupby(['Product_ID'])['Product_ID'].transform('count')\n",
    "pID_count = train.groupby(['Product_ID']).size().to_dict()\n",
    "test['Product_ID_Count'] = test['Product_ID'].apply(lambda x:pID_count.get(x,0))\n",
    "\n",
    "train[\"User_ID_MeanPrice\"] = train.groupby(['User_ID'])['Purchase'].transform('mean')\n",
    "userID_mean_dict = train.groupby(['User_ID'])['Purchase'].mean().to_dict()\n",
    "test['User_ID_MeanPrice'] = test['User_ID'].apply(lambda x:userID_mean_dict.get(x,0))\n",
    "\n",
    "train[\"Product_ID_MeanPrice\"] = train.groupby(['Product_ID'])['Purchase'].transform('mean')\n",
    "productID_mean_dict = train.groupby(['Product_ID'])['Purchase'].mean().to_dict()\n",
    "test['Product_ID_MeanPrice'] = test['Product_ID'].apply(lambda x:productID_mean_dict.get(x,0))\n",
    "\n",
    "train[\"Product_Cat1_MeanPrice\"] = train.groupby(['Product_Category_1'])['Purchase'].transform('mean')\n",
    "pc1_mean_dict = train.groupby(['Product_Category_1'])['Purchase'].mean().to_dict()\n",
    "test['Product_Cat1_MeanPrice'] = test['Product_Category_1'].apply(lambda x:pc1_mean_dict.get(x,0))\n",
    "\n",
    "train[\"Product_Cat2_MeanPrice\"] = train.groupby(['Product_Category_2'])['Purchase'].transform('mean')\n",
    "pc2_mean_dict = train.groupby(['Product_Category_2'])['Purchase'].mean().to_dict()\n",
    "test['Product_Cat2_MeanPrice'] = test['Product_Category_2'].apply(lambda x:pc2_mean_dict.get(x,0))\n",
    "\n",
    "train[\"Product_Cat3_MeanPrice\"] = train.groupby(['Product_Category_3'])['Purchase'].transform('mean')\n",
    "pc3_mean_dict = train.groupby(['Product_Category_3'])['Purchase'].mean().to_dict()\n",
    "test['Product_Cat3_MeanPrice'] = test['Product_Category_3'].apply(lambda x:pc3_mean_dict.get(x,0))\n",
    "\n",
    "userID_25p_dict = train.groupby(['User_ID'])['Purchase'].apply(lambda x:np.percentile(x,25)).to_dict()\n",
    "train['User_ID_25PercPrice'] = train['User_ID'].apply(lambda x:userID_25p_dict.get(x,0))\n",
    "test['User_ID_25PercPrice'] = test['User_ID'].apply(lambda x:userID_25p_dict.get(x,0))\n",
    "\n",
    "userID_50p_dict = train.groupby(['User_ID'])['Purchase'].apply(lambda x:np.percentile(x,50.)).to_dict()\n",
    "train['User_ID_50PercPrice'] = train['User_ID'].apply(lambda x:userID_50p_dict.get(x,0))\n",
    "test['User_ID_50PercPrice'] = test['User_ID'].apply(lambda x:userID_50p_dict.get(x,0))\n",
    "\n",
    "userID_75p_dict = train.groupby(['User_ID'])['Purchase'].apply(lambda x:np.percentile(x,75)).to_dict()\n",
    "train['User_ID_75PercPrice'] = train['User_ID'].apply(lambda x:userID_75p_dict.get(x,0))\n",
    "test['User_ID_75PercPrice'] = test['User_ID'].apply(lambda x:userID_75p_dict.get(x,0))\n",
    "\n",
    "productID_25p_dict = train.groupby(['Product_ID'])['Purchase'].apply(lambda x:np.percentile(x,25)).to_dict()\n",
    "train['Product_ID_25PercPrice'] = train['Product_ID'].apply(lambda x:productID_25p_dict.get(x,0))\n",
    "test['Product_ID_25PercPrice'] = test['Product_ID'].apply(lambda x:productID_25p_dict.get(x,0))\n",
    "\n",
    "productID_50p_dict = train.groupby(['Product_ID'])['Purchase'].apply(lambda x:np.percentile(x,50)).to_dict()\n",
    "train['Product_ID_50PercPrice'] = train['Product_ID'].apply(lambda x:productID_50p_dict.get(x,0))\n",
    "test['Product_ID_50PercPrice'] = test['Product_ID'].apply(lambda x:productID_50p_dict.get(x,0))\n",
    "\n",
    "productID_75p_dict = train.groupby(['Product_ID'])['Purchase'].apply(lambda x:np.percentile(x,75)).to_dict()\n",
    "train['Product_ID_75PercPrice'] = train['Product_ID'].apply(lambda x:productID_75p_dict.get(x,0))\n",
    "test['Product_ID_75PercPrice'] = test['Product_ID'].apply(lambda x:productID_75p_dict.get(x,0))\n",
    "\n",
    "train[\"Product_Category_1_Count\"] = train.groupby(['Product_Category_1'])['Product_Category_1'].transform('count')\n",
    "pc1_count_dict = train.groupby(['Product_Category_1']).size().to_dict()\n",
    "test['Product_Category_1_Count'] = test['Product_Category_1'].apply(lambda x:pc1_count_dict.get(x,0))\n",
    "\n",
    "train[\"Product_Category_2_Count\"] = train.groupby(['Product_Category_2'])['Product_Category_2'].transform('count')\n",
    "pc2_count_dict = train.groupby(['Product_Category_2']).size().to_dict()\n",
    "test['Product_Category_2_Count'] = test['Product_Category_2'].apply(lambda x:pc2_count_dict.get(x,0))\n",
    "\n",
    "train[\"Product_Category_3_Count\"] = train.groupby(['Product_Category_3'])['Product_Category_3'].transform('count')\n",
    "pc3_count_dict = train.groupby(['Product_Category_3']).size().to_dict()\n",
    "test['Product_Category_3_Count'] = test['Product_Category_3'].apply(lambda x:pc3_count_dict.get(x,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train[\"Purchase\"]\n",
    "train.drop([\"Purchase\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    train, train_y, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# first we fit the scaler on the training dataset\n",
    "scaler.fit(train)\n",
    "\n",
    "# then we call the transform method to scale both the training and testing data\n",
    "x_train_scaled = scaler.transform(train)\n",
    "test_scaled=scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14539/14539 - 20s - loss: 6961449.0000 - val_loss: 6125320.0000\n",
      "Epoch 2/200\n",
      "14539/14539 - 20s - loss: 6498350.0000 - val_loss: 6108585.5000\n",
      "Epoch 3/200\n",
      "14539/14539 - 20s - loss: 6415414.5000 - val_loss: 6108112.0000\n",
      "Epoch 4/200\n",
      "14539/14539 - 20s - loss: 6384537.5000 - val_loss: 6292367.0000\n",
      "Epoch 5/200\n",
      "14539/14539 - 20s - loss: 6354053.0000 - val_loss: 6033735.0000\n",
      "Epoch 6/200\n",
      "14539/14539 - 20s - loss: 6328991.5000 - val_loss: 6019710.5000\n",
      "Epoch 7/200\n",
      "14539/14539 - 20s - loss: 6309747.0000 - val_loss: 6012759.0000\n",
      "Epoch 8/200\n",
      "14539/14539 - 20s - loss: 6293335.0000 - val_loss: 5998164.0000\n",
      "Epoch 9/200\n",
      "14539/14539 - 20s - loss: 6273415.0000 - val_loss: 6025114.5000\n",
      "Epoch 10/200\n",
      "14539/14539 - 20s - loss: 6259729.5000 - val_loss: 6005425.5000\n",
      "Epoch 11/200\n",
      "14539/14539 - 20s - loss: 6240197.0000 - val_loss: 5991335.5000\n",
      "Epoch 12/200\n",
      "14539/14539 - 20s - loss: 6229860.5000 - val_loss: 5959888.0000\n",
      "Epoch 13/200\n",
      "14539/14539 - 21s - loss: 6217678.5000 - val_loss: 6058498.0000\n",
      "Epoch 14/200\n",
      "14539/14539 - 21s - loss: 6204724.5000 - val_loss: 6153387.5000\n",
      "Epoch 15/200\n",
      "14539/14539 - 21s - loss: 6202818.5000 - val_loss: 5991824.5000\n",
      "Epoch 16/200\n",
      "14539/14539 - 21s - loss: 6186079.0000 - val_loss: 5936347.0000\n",
      "Epoch 17/200\n",
      "14539/14539 - 23s - loss: 6182787.5000 - val_loss: 6014774.0000\n",
      "Epoch 18/200\n",
      "14539/14539 - 21s - loss: 6173944.5000 - val_loss: 6076158.0000\n",
      "Epoch 19/200\n",
      "14539/14539 - 22s - loss: 6160312.5000 - val_loss: 5932315.0000\n",
      "Epoch 20/200\n",
      "14539/14539 - 24s - loss: 6153495.0000 - val_loss: 5948736.5000\n",
      "Epoch 21/200\n",
      "14539/14539 - 23s - loss: 6150824.0000 - val_loss: 5979632.0000\n",
      "Epoch 22/200\n",
      "14539/14539 - 23s - loss: 6140847.5000 - val_loss: 5939361.0000\n",
      "Epoch 23/200\n",
      "14539/14539 - 22s - loss: 6137272.0000 - val_loss: 5909474.5000\n",
      "Epoch 24/200\n",
      "14539/14539 - 22s - loss: 6132079.0000 - val_loss: 6013152.5000\n",
      "Epoch 25/200\n",
      "14539/14539 - 25s - loss: 6126061.0000 - val_loss: 5987903.5000\n",
      "Epoch 26/200\n",
      "14539/14539 - 25s - loss: 6118466.0000 - val_loss: 6017637.0000\n",
      "Epoch 27/200\n",
      "14539/14539 - 24s - loss: 6117146.0000 - val_loss: 5941375.0000\n",
      "Epoch 28/200\n",
      "14539/14539 - 24s - loss: 6109145.5000 - val_loss: 5904451.5000\n",
      "Epoch 29/200\n",
      "14539/14539 - 21s - loss: 6104842.5000 - val_loss: 5932600.5000\n",
      "Epoch 30/200\n",
      "14539/14539 - 20s - loss: 6097593.5000 - val_loss: 5899533.5000\n",
      "Epoch 31/200\n",
      "14539/14539 - 20s - loss: 6095494.0000 - val_loss: 5966824.0000\n",
      "Epoch 32/200\n",
      "14539/14539 - 20s - loss: 6086892.5000 - val_loss: 5967525.0000\n",
      "Epoch 33/200\n",
      "14539/14539 - 20s - loss: 6083242.0000 - val_loss: 5903906.5000\n",
      "Epoch 34/200\n",
      "14539/14539 - 20s - loss: 6076236.5000 - val_loss: 5926783.0000\n",
      "Epoch 35/200\n",
      "14539/14539 - 20s - loss: 6076987.5000 - val_loss: 5884845.0000\n",
      "Epoch 36/200\n",
      "14539/14539 - 21s - loss: 6074518.5000 - val_loss: 5938028.0000\n",
      "Epoch 37/200\n",
      "14539/14539 - 20s - loss: 6065594.5000 - val_loss: 5904102.0000\n",
      "Epoch 38/200\n",
      "14539/14539 - 20s - loss: 6061619.0000 - val_loss: 5915750.5000\n",
      "Epoch 39/200\n",
      "14539/14539 - 20s - loss: 6058229.0000 - val_loss: 6020963.5000\n",
      "Epoch 40/200\n",
      "14539/14539 - 20s - loss: 6053947.5000 - val_loss: 5902704.5000\n",
      "Epoch 41/200\n",
      "14539/14539 - 20s - loss: 6055500.5000 - val_loss: 5927625.0000\n",
      "Epoch 42/200\n",
      "14539/14539 - 20s - loss: 6047307.0000 - val_loss: 5887647.5000\n",
      "Epoch 43/200\n",
      "Restoring model weights from the end of the best epoch.\n",
      "14539/14539 - 20s - loss: 6044844.0000 - val_loss: 5901515.5000\n",
      "Epoch 00043: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23f89ff2640>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=train.shape[1], activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1)) # Output\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt,loss='mean_squared_error')\n",
    "estp = EarlyStopping(monitor='val_loss', min_delta=0,patience=8, verbose=1, mode='auto',restore_best_weights=True)\n",
    "model.fit(x_train_scaled,train_y,validation_split=0.15,shuffle='True',verbose=2,epochs=200, callbacks=[estp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                1440      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              33792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                16416     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,101,793\n",
      "Trainable params: 1,101,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3 = model.predict(test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(pred3, columns = ['Purchase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl['Purchase']=pred3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(n < 30 for n in dl[\"Purchase\"].values.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.to_csv('deepfinal.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
